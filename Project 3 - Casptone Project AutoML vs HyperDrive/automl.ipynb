{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project.\n",
    "\n",
    "With the help of python notebooks provided in 1st and 2nd project of this nanodegree program ('Optimizing a pipeline in Azure' and 'Operationalizing machine learning'), I have imported following basic dependencies required to complete this project. Any other specific dependecy will be imported as we proceed further in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1610866011944
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.31.0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# CheckIng core SDK version number\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1610866034952
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick-starts-ws-150538\n",
      "aml-quickstarts-150538\n",
      "southcentralus\n",
      "81cefad3-d2c9-4f77-a466-99a7f541c7bb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>AutoML-Exp002</td><td>quick-starts-ws-150538</td><td><a href=\"https://ml.azure.com/experiments/id/c21f9946-7a65-44e8-bc4d-c1721c26a06b?wsid=/subscriptions/81cefad3-d2c9-4f77-a466-99a7f541c7bb/resourcegroups/aml-quickstarts-150538/workspaces/quick-starts-ws-150538&amp;tid=660b3398-b80e-49d2-bc5b-ac1dc93b5254\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Experiment(Name: AutoML-Exp002,\n",
       "Workspace: quick-starts-ws-150538)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an automl experiment in our workspace\n",
    "\n",
    "# initializing a workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')\n",
    "\n",
    "# choosing a name for experiment\n",
    "experiment_name = 'AutoML-Exp002'\n",
    "project_folder = './AutoML-pipeline-project'\n",
    "\n",
    "# creating the experiment\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment.start_logging()\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610866655912
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded........................................................"
     ]
    }
   ],
   "source": [
    "# creating an AMLCompute cluster for running the experiment\n",
    "\n",
    "# importing required dependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choosing a name for our CPU cluster\n",
    "amlcompute_cluster_name = \"CI-AutoML01\"\n",
    "\n",
    "# Verifying that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2', max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "compute_target.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "The dataset that we will be using for this project is the [Heart Failure Prediction](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) dataset from Kaggle, it consist of 12 features.\n",
    "\n",
    "**12 clinical features:**\n",
    "\n",
    "* age - Age\n",
    "\n",
    "* anaemia - Decrease of red blood cells or hemoglobin (boolean)\n",
    "\n",
    "* creatinine_phosphokinase - Level of the CPK enzyme in the blood (mcg/L)\n",
    "\n",
    "* diabetes - If the patient has diabetes (boolean)\n",
    "\n",
    "* ejection_fraction - Percentage of blood leaving the heart at each contraction (percentage)\n",
    "\n",
    "* high_blood_pressure - If the patient has hypertension (boolean)\n",
    "  \n",
    "* platelets - Platelets in the blood (kiloplatelets/mL)\n",
    "\n",
    "* serum_creatinine - Level of serum creatinine in the blood (mg/dL)\n",
    "\n",
    "* serum_sodium - Level of serum sodium in the blood (mEq/L)\n",
    "  \n",
    "* sex - Woman or man (binary)\n",
    "  \n",
    "* smoking - If the patient smokes or not (boolean)\n",
    "\n",
    "* time - Follow-up period (days)\n",
    "\n",
    "\n",
    "We will be predicting the following output:\n",
    "\n",
    "DEATH_EVENT if the patient deceased during the follow-up period (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "key = \"Heart Failure Prediction\"\n",
    "description_text = \"Heart Failure Prediction DataSet for Udacity Project 3\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "        print(\"The dataset is loaded\")\n",
    "\n",
    "if not found:\n",
    "        # Creating Dataset and register it into Workspace\n",
    "        data1 = \"https://raw.githubusercontent.com/RollyAngell/ML-Azure-Udacity/main/Project%203%20-%20Casptone%20Project%20AutoML%20vs%20HyperDrive/heart%20failure%20clinical%20records%20dataset.csv\"\n",
    "        dataset = Dataset.Tabular.from_delimited_files(data1)        \n",
    "        # Registering Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = dataset.to_pandas_dataframe()\n",
    "df_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"DEATH_EVENT\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "Explain why you chose the automl settings and cofiguration you used below.\n",
    "\n",
    "|Setting |Why?|\n",
    "|-|-|\n",
    "|**experiment_timeout_minutes**|We choose 30 minutos that means the experiment will exit after that time.|\n",
    "|**max_concurrent_iterations**|To get result quickly we choose 5 that means that 5 iterations at the same time.|\n",
    "|**primary_metric**|This is the metric that we want to optimize (accuracy). I will use 'AUC_weighted' as 'primary_metric' parameter. AUC means the area under the Receiver Operating Characteristic Curve which plots the relationship between true positive rate and false positive rate. Since our dataset doesn't have high class imbalance, we can use ROC method for judging the performance of a model. I will use AUC_weighted in order to mitigate the effects of whatever little imbalance is there in the dataset. AUC_weighted is the arithmetic mean of the score for each class, weighted by the number of true instances in each class.|\n",
    "|**task**|classification |\n",
    "|**compute_target**|To define the compute cluster we will be using |\n",
    "|**training_data**|It is the training dataset to be used for the experiment.|\n",
    "|**label_column_name**|To specify the dependent variable that we are trying to classify. I will enter 'DEATH_EVENT' as 'label_column_name' parameter.|\n",
    "|**path**|To specify the dependent variable that we are trying to classify |\n",
    "|**enable_early_stopping**|we can choose to terminate the experiment if the score stops improving in the short term. I will enter 'True' as 'enable_early_stopping' parameter.|\n",
    "|**featurization**|To specify the dependent variable that we are trying to classify |\n",
    "|**debug_log**|To specify the dependent variable that we are trying to classify |\n",
    "|**enable_onnx_compatible_models **|To specify the dependent variable that we are trying to classify |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610866769545
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\" : 30,\n",
    "    \"max_concurrent_iterations\" : 5,\n",
    "    \"primary_metric\" : 'AUC_weighted'\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                            task=\"classification\",\n",
    "                            training_data=dataset,\n",
    "                            label_column_name=\"DEATH_EVENT\",\n",
    "                            path='./AutoML-pipeline-project',\n",
    "                            enable_early_stopping=True,\n",
    "                            featurization='auto',\n",
    "                            enable_onnx_compatible_models=True,\n",
    "                            debug_log=\"automl_errors.log\",\n",
    "                            **automl_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868417207
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(config = automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868717293
    }
   },
   "outputs": [],
   "source": [
    "# waiting for completion of remote_run while showing its output\n",
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868834485
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868946612
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve best model from AutoML Run\n",
    "\n",
    "# importing required dependencies\n",
    "import joblib\n",
    "\n",
    "best_automl_run, best_automl_fitted_model = remote_run.get_output()\n",
    "print(best_automl_run)\n",
    "print(best_automl_fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868966185
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "joblib.dump(best_automl_fitted_model,'best_automl_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868971395
    }
   },
   "outputs": [],
   "source": [
    "# getting the details of the best model produced by automl\n",
    "best_automl_run.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610868994344
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def print_model(model,prefix=\"\"):\n",
    "    for step in model.steps:\n",
    "        print(prefix+step[0])\n",
    "        if hasattr(step[1],'estimators') and hasattr(step[1],'weights'):\n",
    "            pprint({'estimators':list(e[0] for e in step[1].estimators), 'weights':step[1].weights})\n",
    "            print()\n",
    "            for estimator in step[1].estimators:\n",
    "                print_model(estimator[1],estimator[0]+'-')\n",
    "        else:\n",
    "            pprint(step[1].get_params())\n",
    "            print()\n",
    "\n",
    "print_model(best_automl_fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610869118916
    }
   },
   "outputs": [],
   "source": [
    "# retrieving the best model as ONNX model\n",
    "best_auto_run, best_onnx_model = remote_run.get_output(return_onnx_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610869141977
    }
   },
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
    "\n",
    "# saving the best model as onnx_model\n",
    "onnx_fl_path = \"./best_model.onnx\"\n",
    "OnnxConverter.save_onnx_model(best_onnx_model, onnx_fl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "gather": {
     "logged": 1610870160746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# registering the best automl model\n",
    "\n",
    "description = 'heart failure predictions'\n",
    "tags = None\n",
    "\n",
    "model = remote_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(remote_run.model_id)\n",
    "\n",
    "# importing required dependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "# loading a curated environment from workspace\n",
    "\n",
    "env = Environment.get(ws, \"AzureML-AutoML\")\n",
    "\n",
    "# specifying scikit-learn as dependency\n",
    "for pip_package in [\"scikit-learn\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "# creating an inference config\n",
    "inference_config = InferenceConfig(entry_script='entry_script.py', environment=env)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, enable_app_insights = True)\n",
    "\n",
    "# naming the service to be deployed\n",
    "aci_service_name = 'automl-heart-failure-predictions'\n",
    "print(aci_service_name)\n",
    "\n",
    "# deploying the model\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(show_output = True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "gather": {
     "logged": 1610870221080
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from numpy import array\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# creating a test sample\n",
    "data = {\"data\": [{\"age\":60.000000,\"anaemia\":0.000000,\"creatinine_phosphokinase\":250.000000,\"diabetes\":0.000000,\"ejection_fraction\":38.000000,\"high_blood_pressure\":0.000000,\"platelets\":262000.000000,\"serum_creatinine\":1.10000,\"serum_sodium\":137.000000,\"sex\":1.000000,\"smoking\":0.00000,\"time\":115.000000}]}\n",
    "td = json.dumps(data)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# sending request to test the deployed webservice\n",
    "resp = requests.post(aci_service.scoring_uri, td, headers=headers)\n",
    "print(resp.json())\n",
    "y_pred = (json.loads(resp.text))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "data = {\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'http://28a21b76-4580-412b-bb9e-fad3a62cf011.southcentralus.azurecontainer.io/score'\n",
    "api_key = '' # Replace this with the API key for the web service\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "gather": {
     "logged": 1610871382502
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# printing the logs of deployed web service\n",
    "dep_logs = aci_service.get_logs()\n",
    "for l in dep_logs.split('\\n'):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "gather": {
     "logged": 1610871657577
    }
   },
   "outputs": [],
   "source": [
    "# deleting a web service\n",
    "aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "gather": {
     "logged": 1610871701038
    }
   },
   "outputs": [],
   "source": [
    "# delete compute cluster\n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
